{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bb6130d-e340-42eb-8669-a9082b144e35",
   "metadata": {},
   "source": [
    "# Fashion Image Captioner\n",
    "\n",
    "This notebook finetunes a general visual language model (VLM) that can caption images to be able to generate more specific search-relevant descriptions of images from thrifted fashion item inventories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347e8705-8b8c-4fe8-94cc-ca8c33473996",
   "metadata": {},
   "source": [
    "# Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4493dd1d-2e20-470e-ba0f-5481b52abe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nbdev: EXPORT\n",
    "#Install packages needed first time in Environment\n",
    "if False:\n",
    "    # !pip install pandas\n",
    "    # !pip install scikit-learn\n",
    "    # !pip install torch torchvision\n",
    "    # !pip install transformers\n",
    "    !pip install datasets #Hugging Face\n",
    "    !pip install tqdm\n",
    "    !pip install Pillow\n",
    "    !pip install peft\n",
    "    !pip install rouge_score\n",
    "    !pip install evaluate\n",
    "    # !apt-get install nvtop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a56973-6f74-4f49-9d69-4f1e420e5e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "\n",
    "#General\n",
    "import os\n",
    "import gc\n",
    "\n",
    "#Standard ML\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#Data Visualization\n",
    "from torchvision.transforms import ToPILImage\n",
    "from PIL import Image as pilImage\n",
    "from IPython.display import display\n",
    "\n",
    "#HuggingFace Libraries\n",
    "from datasets import Dataset as hfDataset\n",
    "from datasets import Image\n",
    "from accelerate import Accelerator\n",
    "\n",
    "#The Model\n",
    "from transformers import Blip2ForConditionalGeneration #visual language model (VLM) can do multimodal tasks like captioning or answering questions about images.\n",
    "from transformers import Blip2Processor\n",
    "\n",
    "#Extras\n",
    "from tqdm import tqdm #cute loading percent bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c4231e-f9b2-4e26-82ca-c505bf5b75bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU Availability\n",
    "# Device will be reset automatically later by accelerator, but this is a good sanity check to know what we should be working with\n",
    "\n",
    "# Check for MPS availability\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Use Apple's MPS\n",
    "    print(f\"Using torch version {torch.__version__}\")\n",
    "    print(\"Found MPS is available for use\") # Is MPS even available? macOS 12.3+\n",
    "    print(f\"Current version on Pytorch was built with MPS activation: {torch.backends.mps.is_built()}\") # Was the current version of PyTorch built with MPS activated?\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    nGPUs = torch.cuda.device_count()\n",
    "    print(f\"Found {nGPUs} cuda GPUs available for use\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU device found, running on cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1f11cf-b5c2-48c4-9948-452298fd995a",
   "metadata": {},
   "source": [
    "# Import Data\n",
    "\n",
    "To use as-is, a root project folder should contain\n",
    "* a subproject folder containing this notebook (can have any name)\n",
    "* a data folder, called Data, containing\n",
    "  *  DB_FILENAME: a csv file with at least two columns\n",
    "     *   IMAGE_PATH_COL: contains image names\n",
    "     *   CAPTION_COL: contains image cpations\n",
    "  * IMAGE_FOLDERNAME: A folder containing images with the names referenced in IMAGE_PATH_COL\n",
    " \n",
    "The logic for this sturcture is that there can be multiple subprojects that need to access the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281e6f4f-4ef2-4926-a598-f0edc2fe4807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET IMAGE LOCATION INFO\n",
    "\n",
    "#Specify Data Locations\n",
    "CSV_FILENAME_g = 'large_fashion_descriptions.csv' #'ThredupData.csv'\n",
    "CAPTION_COL_g = \"description\"\n",
    "IMAGE_PATH_COL_g = \"filename\"\n",
    "IMAGE_FOLDERNAME_g = 'Extracted_Images' #'StanfordJpegs'\n",
    "\n",
    "DATA_FOLDER_g = os.path.join('..', 'Data') #'..' means the folder one up from current notebook's folder, which happens to be where I have my data folder\n",
    "IMAGE_FOLDER_g = os.path.join(DATA_FOLDER_g, IMAGE_FOLDERNAME_g)\n",
    "CSV_FILEPATH_g = os.path.join(DATA_FOLDER_g, CSV_FILENAME_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c7b9af-17b6-48ff-a717-9db07a860731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT RAW DATA TO DATAFRAME\n",
    "\n",
    "def getDatasetDF(csv_filepath = CSV_FILEPATH_g, image_folder = IMAGE_FOLDER_g):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create dataframe based on csv file\n",
    "    returns data_df which has infomration to eventually create two columns: captions (string descriptions of images) and images (paths to image files) and c\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(csv_filepath) == 0:\n",
    "        return\n",
    "    data_df = read_csv(csv_filepath)\n",
    "    data_df.index.name = \"df_id\" #name index to be used for matching datapoints across imported df, hugging face train/test sets and torch dataloaders\n",
    "    \n",
    "    return data_df # Should have \n",
    "    \n",
    "\n",
    "data_df_g = getDatasetDF()\n",
    "data_df_g.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe7414d-5f32-4804-a1cf-e195e920b62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optionally manually edit dataframe to explore effect of individual changes\n",
    "\n",
    "changeDataManually_g = False\n",
    "\n",
    "def changeRows(data_df = data_df_g, colToChange = CAPTION_COL_g, should_change=changeDataManually_g):\n",
    "    if not should_change: return data_df\n",
    "    else:\n",
    "        colToChange = \"Captions\"\n",
    "        changeDict = {\n",
    "            846: \"a black ankleboot with fringe detailing on the side\",\n",
    "            4815: \"a yellow beanie on a white background\",\n",
    "            5093: \"a cage-style heel with floral print\",\n",
    "            2835: \"a black leather heeled boot with a pointed toe\"   \n",
    "            }\n",
    "        \n",
    "        for itemId, content in changeDict.items():\n",
    "            data_df.loc[itemId,colToChange] = content\n",
    "        return data_df\n",
    "\n",
    "\n",
    "data_df_g = changeRows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a2936c-58be-4a75-a6ce-c44d7d571d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILTER DATAFRAME TO JUST 3 COLUMNS: df_id, image_paths, captions\n",
    "\n",
    "def imagePathColCreator(data_df, image_folder, image_path_col):\n",
    "    #image_paths = list(os.path.join(image_folder,('item'+str(fileId))) for fileId in data_df[\"Item_Id\"]) #From original ThredupData.csv Use ItemId column to create list of paths to image files\n",
    "    print(data_df.columns)\n",
    "    image_paths = list(os.path.join(image_folder,filePath) for filePath in data_df[image_path_col]); #From Esra's LLM generated caption dataset (files in folder and spreadhseet adjusted to include .jpg in name)\n",
    "    return image_paths\n",
    "\n",
    "def captionColCreator(data_df, caption_col):\n",
    "    #captions = list(data_df[\"Category_Type\"] + \", \" + data_df[\"Description\"]) #If using original ThredupData.csv, use Description column to create list of captions for each image  \n",
    "    captions = data_df[caption_col] #From Esra's LLM generated caption dataset\n",
    "    return captions\n",
    "\n",
    "def createImageAndCaptionDf(data_df = data_df_g, image_folder = IMAGE_FOLDER_g, image_path_col= IMAGE_PATH_COL_g, caption_col = CAPTION_COL_g ):\n",
    "\n",
    "    #Create image paths\n",
    "    data_df[\"image_paths\"] = imagePathColCreator(data_df, image_folder, image_path_col)\n",
    "\n",
    "    #Create caption paths\n",
    "    data_df[\"captions\"] = captionColCreator(data_df, caption_col)\n",
    "     \n",
    "    #Discard rows without valid image paths\n",
    "    print(f\"Original samples in dataframe: {len(data_df)}\")\n",
    "    valid_rows = [ind for ind, image_path in enumerate(data_df[\"image_paths\"]) if os.path.exists(image_path)] #Get rid of any image paths that do not point to actual image files\n",
    "    invalid_rows = [ind for ind in data_df.index if ind not in valid_rows]\n",
    "\n",
    "    invalid_data_df = data_df.iloc[invalid_rows]\n",
    "    print(f\"Samples with invalid image paths in dataframe: {len(invalid_data_df)}\")\n",
    "    data_df = data_df.iloc[valid_rows]\n",
    "    print(f\"Samples with valid image paths in dataframe: {len(data_df)}\")\n",
    "\n",
    "    #Discard all other columns\n",
    "    data_df = data_df[[\"captions\", \"image_paths\"]]\n",
    "\n",
    "    return data_df, invalid_data_df\n",
    "\n",
    "data_df_g, invalid_paths_df_g = createImageAndCaptionDf()\n",
    "\n",
    "data_df_g.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ee6ecd-eaae-44ee-add6-b766d53c1b56",
   "metadata": {},
   "source": [
    "# Separate Train/Valid/Test Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722e8780-966c-4a2d-9865-051cc6ff2594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Toggles\n",
    "\n",
    "VALIDATION_PORTION_g = 0.2\n",
    "TEST_PORTION_g = .1\n",
    "SEED_g = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3772b3-1b6e-4e09-9b92-024d3b2797ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for train/validation/test dataframes, huggingface datasets, and torch data loaders\n",
    "train_df_g = None\n",
    "train_hfset_g = None\n",
    "train_dataloader_g = None\n",
    "\n",
    "validation_df_g = None\n",
    "validation_hfset_g = None\n",
    "validation_dataloader_g = None\n",
    "\n",
    "test_df_g = None\n",
    "test_hfset_g = None\n",
    "test_dataloader_g = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc58c676-beef-43a1-a0f6-e3f8179963f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Populate train/validation/test dataframes\n",
    "\n",
    "# Split off training data\n",
    "train_df_g, test_and_validation_df_g = train_test_split(data_df_g, test_size=(VALIDATION_PORTION_g + TEST_PORTION_g), random_state=SEED_g)\n",
    "\n",
    "#Split remaining into validation and test dats\n",
    "validation_df_g, test_df_g = train_test_split(test_and_validation_df_g, test_size=TEST_PORTION_g, random_state=SEED_g)\n",
    "\n",
    "print(f\"\"\"\n",
    "Train Size: {len(train_df_g)},\n",
    "Validation Size: {len(validation_df_g)},\n",
    "Test Size: {len(test_df_g)} \n",
    "\"\"\")\n",
    "train_df_g.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4808be69-12da-4575-aa49-1b0c6dd39a24",
   "metadata": {},
   "source": [
    "# Create Hugging Face Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d09e04d-6c31-4333-9028-5589f1df705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train, valid, test huggingface datasets\n",
    "\n",
    "def createHuggingfaceDataset(data_df):\n",
    "    image_paths = list(data_df[\"image_paths\"])\n",
    "    captions = list(data_df[\"captions\"])\n",
    "    dataframe_ids = data_df.index.tolist()\n",
    "    \n",
    "    #Create HuggingFace Dataset from python dict containing two lists, image paths and captions\n",
    "    dataset_hf = hfDataset.from_dict(\n",
    "                                {\n",
    "                                    \"image\": image_paths,\n",
    "                                    \"text\": captions,\n",
    "                                    \"df_id\": dataframe_ids\n",
    "                                }\n",
    "                            )\n",
    "    dataset_hf = dataset_hf.cast_column(\"image\", Image()) #Cast the image column to image data type so can be linked files can be accessed as images\n",
    "\n",
    "    #Delete variables to free up memory (may not be necessary)\n",
    "    del [image_paths, captions, data_df, dataframe_ids]\n",
    "    gc.collect() #collect garbage \n",
    "    \n",
    "    return dataset_hf\n",
    "\n",
    "train_hfset_g = createHuggingfaceDataset(train_df_g)\n",
    "print(\"training hugging face dataset created\")\n",
    "\n",
    "validation_hfset_g = createHuggingfaceDataset(validation_df_g)\n",
    "print(\"validation hugging face dataset created\")\n",
    "\n",
    "test_hfset_g = createHuggingfaceDataset(test_df_g)\n",
    "print(\"test hugging face dataset created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c118200a-464e-4797-a285-93bd6f10172a",
   "metadata": {},
   "source": [
    "# Make Functions to Go Btwn df and hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87997b9f-b792-4a75-8e0b-983a8c16db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function for selecting train/test/valid dataset\n",
    "\n",
    "def select_df_hf(segment,\n",
    "                   train_df = train_df_g,\n",
    "                   train_hfset = train_hfset_g,\n",
    "                   validation_df = validation_df_g,\n",
    "                   validation_hfset = validation_hfset_g,\n",
    "                   test_df = test_df_g,\n",
    "                   test_hfset = test_hfset_g):\n",
    "\n",
    "    if segment == \"train\":\n",
    "        return  train_df, train_hfset\n",
    "    elif segment == \"validation\":\n",
    "        return  validation_df, validation_hfset\n",
    "    elif segment == \"test\":\n",
    "        return  test_df, test_hfset\n",
    "\n",
    "#train_df, train_hfset = select_df_hf(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29434e8e-713f-4531-8ba1-251314bab836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table with df_id, hf_segment_id for conversion\n",
    "\n",
    "def create_uid_df(train_hfset, validation_hfset, test_hfset):\n",
    "\n",
    "    uid_df = DataFrame(columns = [\"df_id\", \"segment\", \"hf_segment_id\"])\n",
    "    \n",
    "    hf_segments = {\n",
    "        \"train\": train_hfset,\n",
    "        \"validation\": validation_hfset,\n",
    "        \"test\": test_hfset\n",
    "    }\n",
    "    \n",
    "    for hf_segment_name, hf_segment in hf_segments.items():\n",
    "        for row_id, hf_item in enumerate(hf_segment):\n",
    "            item_id = hf_item[\"df_id\"]\n",
    "            uid_df.loc[item_id, \"df_id\"] = item_id\n",
    "            uid_df.loc[item_id, \"segment\"] = hf_segment_name\n",
    "            uid_df.loc[item_id,\"hf_segment_id\"] = row_id\n",
    "\n",
    "    return uid_df\n",
    "\n",
    "uid_df_g = create_uid_df(train_hfset_g, validation_hfset_g, test_hfset_g)\n",
    "uid_df_g.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9288843-b8e1-444f-a650-cf6fe3ba83c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert from hf_id to uid\n",
    "\n",
    "def get_uid_from_hf_segment_id(segment, hf_segment_id, uid_df = uid_df_g):\n",
    "    df_id = uid_df.loc[(uid_df[\"segment\"] == segment) & (uid_df[\"hf_segment_id\"] == hf_segment_id), \"df_id\"]\n",
    "    assert len(df_id) == 1\n",
    "    uid = df_id.values[0]\n",
    "    return uid\n",
    "\n",
    "#Example \n",
    "segment_g = \"train\"\n",
    "hf_id_g = 0\n",
    "print(f\"The uid of hf {segment_g}, ex {hf_id_g}:\")\n",
    "print(get_uid_from_hf_segment_id(segment_g, hf_id_g))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd65ac6b-2638-474b-b97e-4cc64f150b2e",
   "metadata": {},
   "source": [
    "# View Labeled Data Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32fce5b-32ad-4895-827f-daae680333aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select image to view huggingface and original dataframe data\n",
    "\n",
    "segment_to_view_g = \"train\"\n",
    "segment_ind_g = 6 #index of image to look at in train/validation/test segment\n",
    "\n",
    "def view_labeled_data_example(segment = segment_to_view_g, hf_id = segment_ind_g):\n",
    "\n",
    "        dfset, hfset = select_df_hf(segment)\n",
    "        df_id = get_uid_from_hf_segment_id(segment, hf_id)\n",
    " \n",
    "        #hf image\n",
    "        image = hfset[hf_id][\"image\"].resize((252,252))\n",
    "        print(\"HUGGING FACE IMAGE:\")\n",
    "        display(image)\n",
    "        \n",
    "        #hf caption\n",
    "        print(f'HUGGING FACE CAPTION:\\n{hfset[hf_id][\"text\"]}\\n')\n",
    "        \n",
    "        #original dataframe info\n",
    "        print(f'ORIGINAL DF INFO:\\n{dfset.loc[df_id]}')\n",
    "\n",
    "view_labeled_data_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9314f84a-dee9-49e6-88ca-720e6df6e956",
   "metadata": {},
   "source": [
    "# Import Datasets to Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a104297-402e-4474-93fd-713e85baf5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Pytorch Dataset Class for input o Dataloaders\n",
    "\n",
    "class ImageCaptioningDataset(Dataset): #this class inherits functionalities and structure of the pytorch Dataset class\n",
    "    \n",
    "    def __init__(self, huggingFaceDataset, processor):       \n",
    "        self.dataset = huggingFaceDataset\n",
    "        self.processor = processor\n",
    "        #self.new_size = (256,256)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        item = self.dataset[idx] #each item in self.dataset has an image and text\n",
    "        \n",
    "        #process image (returns a dictionary with a single item containing the processed image)\n",
    "        encoding = self.processor(images = item[\"image\"], #get image\n",
    "                                  #.resize(self.new_size), #resize\n",
    "                                  #padding = \"max_length\", #set padding\n",
    "                                  return_tensors = \"pt\" #return as pytorch tensor dtype\n",
    "                                 )\n",
    "        \n",
    "        # remove batch dimension\n",
    "        encoding = {key: value.squeeze() for key, value in encoding.items()} #\n",
    "\n",
    "        #add text info\n",
    "        encoding[\"text\"] = item[\"text\"]\n",
    "\n",
    "        # Include the original index (from hugging face) in the returned data\n",
    "        encoding['hf_idx'] = idx\n",
    "        \n",
    "        return encoding\n",
    "\n",
    "    def get_original_image(self, idx):\n",
    "        #This method gets the original image from the Hugging Face dataset for a given index.\n",
    "        return self.dataset[idx][\"image\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a07083-a044-4135-97b4-0f3e82bd50e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Processor which does image and caption pre-processing\n",
    "# (used in creation of pytorch dataset and collate functions)\n",
    "\n",
    "def getProcessor():\n",
    "    pretraining = 'Salesforce/blip2-opt-2.7b'\n",
    "    processor = Blip2Processor.from_pretrained(pretraining)\n",
    "    return processor\n",
    "\n",
    "processor_g = getProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d13a8a-da66-4585-8361-ea4bb6f13585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, processor = processor_g):\n",
    "\n",
    "    #batch is a list of examples (dictionaries), where each example has the same keys (image and text)\n",
    "    \n",
    "    # pad the tokenized_captions and attention_mask\n",
    "    processed_batch = {}\n",
    "    \n",
    "    item_keys = batch[0].keys()\n",
    "    \n",
    "    for key in item_keys: #only have to iterate over keys from first example in batch, bc all examples have same keys\n",
    "        if key == \"pixel_values\":\n",
    "            processed_batch[key] = torch.stack([item[key] for item in batch]) #stack images from all examples in batch\n",
    "        elif key == \"text\":\n",
    "            #tokenize the caption (ie break up caption into chunks and convert each chunk to a number using look up table)\n",
    "            text_inputs = processor.tokenizer(\n",
    "                                            [item[\"text\"] for item in batch], \n",
    "                                            padding = \"max_length\" ,#True, \n",
    "                                            max_length = 50, #9, #16\n",
    "                                            return_tensors = \"pt\",\n",
    "                                            truncation = True,\n",
    "                                            )\n",
    "            processed_batch[\"tokenized_captions\"] = text_inputs[\"input_ids\"] #input_ids is conventionally used in the Hugging Face Transformers library and other NLP frameworks to refer to the tokenized representation of text inputs. This naming convention is primarily for consistency and clarity within the context of language models and their input processing\n",
    "            processed_batch[\"attention_mask\"] = text_inputs[\"attention_mask\"]\n",
    "        elif key == \"hf_idx\":\n",
    "            processed_batch[key] = [item[key] for item in batch]\n",
    "        else:\n",
    "            print(f\"{key} KEY NOT ACOUNTED FOR !!\")\n",
    "    \n",
    "    return processed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3743e72-dfd2-465a-b1bc-6ab0bb6d463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE PYTORCH DATA LOADERS\n",
    "\n",
    "BATCH_SIZE_g = 16 #(32+ Too Big on my Vast.AI instance) #56 #64 #128 #512 #1024\n",
    "processor_g = getProcessor()\n",
    "\n",
    "train_dataloader_g = DataLoader(dataset = ImageCaptioningDataset(train_hfset_g, processor_g), \n",
    "                              shuffle = True, \n",
    "                              batch_size = BATCH_SIZE_g, \n",
    "                              collate_fn = collate_fn,\n",
    "                             )\n",
    "\n",
    "validation_dataloader_g = DataLoader(dataset = ImageCaptioningDataset(validation_hfset_g, processor_g), \n",
    "                               batch_size = BATCH_SIZE_g, \n",
    "                               collate_fn = collate_fn,\n",
    "                              )\n",
    "\n",
    "test_dataloader_g = DataLoader(dataset = ImageCaptioningDataset(validation_hfset_g, processor_g), \n",
    "                               batch_size = BATCH_SIZE_g, \n",
    "                               collate_fn = collate_fn,\n",
    "                              )\n",
    "\n",
    "# del train_hfset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c5226d-9d22-40ec-bac9-47f7cc9e0506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_dataset(segment,\n",
    "                   train_dataloader = train_dataloader_g,\n",
    "                   validation_dataloader = validation_dataloader_g,\n",
    "                   test_dataloader = test_dataloader_g):\n",
    "\n",
    "    selected_df, selected_hfset = select_df_hf(segment)\n",
    "    if segment == \"train\":\n",
    "        return  selected_df, selected_hfset, train_dataloader\n",
    "    elif segment == \"validation\":\n",
    "        return  selected_df, selected_hfset, validation_dataloader\n",
    "    elif segment == \"test\":\n",
    "        return  selected_df, selected_hfset, test_dataloader\n",
    "\n",
    "#train_df, train_hfset, train_dataloader = select_dataset(\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9792db4f-bfcf-495e-ba92-76e325f4ab10",
   "metadata": {},
   "source": [
    "# Get Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6879c4a8-031d-404a-98d6-f28e5fc657aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD MODEL\n",
    "model_g = Blip2ForConditionalGeneration.from_pretrained(\"ybelkada/blip2-opt-2.7b-fp16-sharded\", \n",
    "                                                      torch_dtype = torch.float32\n",
    "                                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bcb7ba-b9ec-47ac-98b3-e17b0d9a08ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD LoRA CONFIG\n",
    "\n",
    "from peft import LoraConfig, get_peft_model   \n",
    "    \n",
    "config_g = LoraConfig(\n",
    "    r = 10, #32, #10, #18, #16, # Rank size determined number of trainable parameters.\n",
    "    lora_alpha = 32, # scaling factor sets magnitude LoRA updates have on the original weights\n",
    "    lora_dropout = 0.05, #percent of cells not enabled at any time\n",
    "    bias = \"none\",\n",
    "    target_modules = [\"q_proj\", \"k_proj\"] #focus model's adaptation on the attention mechanism\n",
    ")\n",
    "\n",
    "model_g = get_peft_model(model_g, config_g) #peft model is the hugging face name for their library that uses LORA \"parameter efficient fine-tuning\"\n",
    "    \n",
    "model_g.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb2196f-69d0-4b35-ac70-7a10950d528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZER\n",
    "lr_g = 1e-4 #2e-4 #5e-4 #8e-4\n",
    "optimizer_g = torch.optim.Adam(model_g.parameters(), lr = lr_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ce774c-6fbb-4201-b0d0-ab7a569e98fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACCELERATOR\n",
    "# optimizes training and evaluation loops for the available hardware in a device agnostic way,\n",
    "# eliminating need to use .to(device) or wrap model with torch.nn.DataParallel for multi-GPU training\n",
    "\n",
    "accelerator_g = Accelerator() #hugging face optimization handles device placement and optimizes training speed\n",
    "model_g, optimizer_g, train_dataloader_g, validation_dataloader_g  = accelerator_g.prepare(model_g, optimizer_g, train_dataloader_g, validation_dataloader_g)\n",
    "\n",
    "device_g = accelerator_g.device\n",
    "print(f'Device: {device_g}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9770396-0757-4ade-815c-7e2bc0ad0c9d",
   "metadata": {},
   "source": [
    "# Set up to Compare Before and After Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c128614d-23b9-48bc-97a7-527ee041900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUBFUNCTIONS TO COMPARE LABELS, UNTUNED AND TUNED CAPTIONS\n",
    "\n",
    "def getSubBatch(dataloader, nSamples):\n",
    "    nSamples = min(nSamples, len(dataloader.dataset))\n",
    "    batch_iterator = iter(dataloader) # Create an iterator from the DataLoader\n",
    "    batch = next(batch_iterator) # Fetch full batch\n",
    "    subset_batch = {key: value[:nSamples] for key, value in batch.items()}\n",
    "    n_samples = len(subset_batch)\n",
    "    return subset_batch, n_samples \n",
    "\n",
    "def unpackBatch(batch):\n",
    "    tokenized_captions = batch[\"tokenized_captions\"]\n",
    "    pixel_values = batch[\"pixel_values\"].to(torch.float32).to(device_g)\n",
    "    hf_indices = batch['hf_idx']\n",
    "    return hf_indices,tokenized_captions, pixel_values\n",
    "\n",
    "def getModelPredictions(model, processor, pixel_values):\n",
    "    model.eval()\n",
    "    predictions = model.generate(pixel_values, max_new_tokens = 100)#9 #16 #Here the predictions are text captions written in number encoded words (LUT)\n",
    "    predictions = processor.batch_decode(predictions, skip_special_tokens = True) #now we're in english\n",
    "    return predictions\n",
    "\n",
    "def decipherLabels(processor, tokenized_captions):\n",
    "    labels = processor.batch_decode(tokenized_captions, skip_special_tokens = True)\n",
    "    return labels\n",
    "\n",
    "def getImage(hf_dataset, hf_ind):\n",
    "    return hf_dataset[hf_ind][\"image\"].resize((252, 252))\n",
    "\n",
    "def showImage(image):\n",
    "    display(image.resize((252,252)))\n",
    "\n",
    "def showCaptionComparison(df_id, hf_ind, image, prediction_untuned, prediction_tuned, label):\n",
    "    print(f\"DF Index:{df_id}, Segment index: {hf_ind}\")\n",
    "    showImage(image)\n",
    "    print(f\"UNTUNED CAPTION: {prediction_untuned}\\n\")\n",
    "    print(f\"TUNED CAPTION: {prediction_tuned}\\n\")\n",
    "    print(f\"DESIRED CAPTION: {label}\\n\")\n",
    "    print('==========\\n\\n') \n",
    "\n",
    "def showCaptionComparisons(segment, hfset, hf_indices, predictions_untuned, predictions_tuned, labels):\n",
    "    for demo_ind,label in enumerate(predictions_untuned):\n",
    "        hf_ind = hf_indices[demo_ind]\n",
    "        df_id = get_uid_from_hf_segment_id(segment, hf_ind)\n",
    "        image = getImage(hfset, hf_ind)\n",
    "        showCaptionComparison(df_id, hf_ind, image, predictions_untuned[demo_ind], predictions_tuned[demo_ind], labels[demo_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f0a402-a6f3-4476-909a-9451a91a13f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareCaptionsBefore(segment, maxSamples, model=model_g, processor=processor_g ):\n",
    "    #Get dataset segment (as df, hfset and dataloader)\n",
    "    df, hfset, dataloader = select_dataset(segment)\n",
    "\n",
    "    #Get images and labels from segment dataloader\n",
    "    subset_batch, n_samples = getSubBatch(dataloader, maxSamples)\n",
    "    hf_indices, tokenized_captions, pixel_value_inputs = unpackBatch(subset_batch)\n",
    "\n",
    "    #Get predicted captions by applying model\n",
    "    predictions_untuned = getModelPredictions(model, processor, pixel_value_inputs)\n",
    "    predictions_tuned = predictions_untuned\n",
    "\n",
    "    #Get desired captions from labels\n",
    "    labels = decipherLabels(processor, tokenized_captions)\n",
    "\n",
    "    #Show comparison\n",
    "    showCaptionComparisons(segment, hfset, hf_indices, predictions_untuned, predictions_tuned, labels)\n",
    "    \n",
    "    return segment, hf_indices, pixel_value_inputs, predictions_untuned, labels\n",
    "\n",
    "def compareCaptionsAfter(comparison_segment,\n",
    "                         comparison_hf_indices,\n",
    "                         comparison_pixel_value_inputs,\n",
    "                         comparison_predictions_untuned,\n",
    "                         comparison_labels,\n",
    "                         model=model_g,\n",
    "                         processor=processor_g):\n",
    "\n",
    "    #Get hfset for displaying images\n",
    "    _, hfset, _ = select_dataset(comparison_segment)\n",
    "\n",
    "    #Get new predictions\n",
    "    predictions_tuned = getModelPredictions(model, processor, comparison_pixel_value_inputs)\n",
    "    \n",
    "    #Show comparison\n",
    "    showCaptionComparisons(comparison_segment, hfset, comparison_hf_indices, comparison_predictions_untuned, predictions_tuned, comparison_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d490efcf-a555-4f87-959a-18a6730f7f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at predictions before training\n",
    "segment_g = \"validation\"\n",
    "n_samples_g = 10\n",
    "\n",
    "comparison_segment_g, comparison_hf_indices_g, comparison_pixel_value_inputs_g, comparison_predictions_untuned_g, comparison_labels_g  = compareCaptionsBefore(segment_g, n_samples_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f05192-5952-4c3b-a9e7-09516d87c9ae",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a157d0-1af8-48d6-945d-291b9400d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define default values\n",
    "\n",
    "n_epochs_g = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebdf79e-8a18-460e-b26e-a87b8e600488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, n_epochs = n_epochs_g, train_dataloader = train_dataloader_g, optimizer = optimizer_g, accelerator = accelerator_g): \n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"{epoch= }\")\n",
    "        \n",
    "        epoch_accumulated_loss = 0\n",
    "        nBatches = 0        \n",
    "            \n",
    "        model.train()\n",
    "        for batch in tqdm(train_dataloader): #batch is a dictionary\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #Get X and y\n",
    "            tokenized_captions = batch.pop(\"tokenized_captions\") #!!!*** pops instead of indexing to save memory (on old hardware)\n",
    "            pixel_values = batch.pop(\"pixel_values\").to(torch.float32)\n",
    "\n",
    "            #Get predictions\n",
    "            outputs = model(input_ids = tokenized_captions, #input_ids is conventional HF name for tokenized text\n",
    "                            pixel_values = pixel_values,\n",
    "                            labels = tokenized_captions)\n",
    "            \n",
    "            #Calculate loss and train\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()   \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            epoch_accumulated_loss += loss.item()\n",
    "            nBatches+=1\n",
    "    \n",
    "        print(f\"Average Loss: {epoch_accumulated_loss/nBatches}\")\n",
    "        \n",
    "        print(\"!!!***!!! UNCOMMENT VALIDATION CHECK ONCE IMPLEMENTED\")        \n",
    "        # model.eval()\n",
    "        \n",
    "        # rouge_score_aggregator = scoring.BootstrapAggregator()\n",
    "        # bleu_score_aggregator = scoring.BootstrapAggregator()\n",
    "    \n",
    "        # for batch in tqdm(validation_dataloader):\n",
    "    \n",
    "        #     tokenized_captions = batch.pop(\"tokenized_captions\")#.to(device)\n",
    "        #     pixel_values = batch.pop(\"pixel_values\").to(torch.float32)\n",
    "        \n",
    "        #     predictions = model.generate(pixel_values, max_length = 10)#9 #16 #Here the predictions are text captions written in number encoded words (LUT)\n",
    "        #     predictions = processor.batch_decode(predictions, skip_special_tokens = True) #now we're in english\n",
    "        #     labels = processor.batch_decode(tokenized_captions, skip_special_tokens = True) #these are the pre-set captions (decoded from numbers)\n",
    "    \n",
    "        #     eval_metric = compute_metrics(predictions, labels) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51da2df-9160-407b-a3d1-03794e55309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs_g = n_epochs_g #Set this to a new number if don't want old n_epochs_g\n",
    "train_model(model_g, n_epochs_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3ccc3e-09ee-47e2-a55a-34851a68b1be",
   "metadata": {},
   "source": [
    "# Look at Before and After"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a2838c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3be3aa-8d22-4aa1-82f3-1fc31d997367",
   "metadata": {},
   "outputs": [],
   "source": [
    "compareCaptionsAfter(comparison_segment_g,\n",
    "                         comparison_hf_indices_g,\n",
    "                         comparison_pixel_value_inputs_g,\n",
    "                         comparison_predictions_untuned_g,\n",
    "                         comparison_labels_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ce04cd-7ff6-4f29-8b29-cec5fc27614a",
   "metadata": {},
   "source": [
    "## KARINA TO DO:\n",
    "\n",
    "* commit that refactored for clarity and addressed bug when displaying before and after demo data\n",
    "* make a similar items benchmark csv of deduped data so can use this data as a test once have model trained\n",
    "* don't need a multi-GPU instance unless change code to be able to take advantage\n",
    "* implement Regi's nbdev approach to organize jupyter notebooks then delete Regis scratchpad from this notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dad14c2-ebd1-4c12-b574-9945794ffd5f",
   "metadata": {},
   "source": [
    "## REGIS SCRATCHPAD\n",
    "\n",
    "Command line to get list of deduplicated files in a folder: ffor file in *; do md5sum \"$file\"; done | sort | awk '!seen[$1]++ {print $2}\n",
    "\n",
    "https://nbdev.fast.ai # from jeremy howard see youtube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aad966-816e-475f-bcdd-0b2d169cd682",
   "metadata": {},
   "source": [
    "### dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281ff26f-f315-4cd4-aa5f-7f26c3972f03",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a9f0a-3a11-4746-a7c5-0686be79c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # nbdev: EXPORT\n",
    "# def load_dataset(path):\n",
    "#     # return a dataset\n",
    "#     pass\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1e326f-576b-4790-ac89-53e77ff0af01",
   "metadata": {},
   "source": [
    "### tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100cff0b-241f-497f-abc5-baf2a3aa1f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a folder\n",
    "# #create 3 images inside\n",
    "# ds = load_dataset('./test123')\n",
    "# assert len(ds) == 3\n",
    "# assert ds[0]['filename'] == 'first.jpg'\n",
    "# assert ds.isin(..) != ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a56799-9e03-4f62-bcfe-8afe6ab16632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
