{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e11b59f",
   "metadata": {},
   "source": [
    "# IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "optimum-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/env python3\n",
    "# !pip install tqdm\n",
    "# ! pip install selenium\n",
    "# ! pip install bs4\n",
    "# ! pip install alive_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5856709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from alive_progress import alive_bar\n",
    "import re\n",
    "import requests # to get image from the web\n",
    "import shutil # to save it locally\n",
    "from os.path import exists\n",
    "from os.path import expanduser\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchWindowException\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6469d242",
   "metadata": {},
   "source": [
    "# SETTINGS\n",
    "\n",
    "- NOTE: If we want 10,000 items from four categories (shirts, sweaters, dresses, outwerwear), we need about 2,500 per category\n",
    "- so set max_products = 2,500\n",
    "- max_pages = 21 (bc 120 items per pages as default)\n",
    "- for each front_page_url, make sure to save a copy of the link used here and it should have items ordered from newest to oldest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d05ad08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = \"C:/Users/Esra/Desktop/Deep_Learning/Image_Classification/Fashion/Classify_ThreadUp_Images/data/threadup/dress/\"\n",
    "max_products = \"all\" #set to \"all\" if want to take all products on scraped pages\n",
    "front_page_url = \"https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=1\"\n",
    "max_pages = 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee64d31",
   "metadata": {},
   "source": [
    "# SCRAPE PAGE FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "845d19d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(my_url):\n",
    "    print(\"Scraping URL:\", my_url)\n",
    "    firefox_options = webdriver.FirefoxOptions()\n",
    "    firefox_options.add_argument('--headless')\n",
    "    firefox_options.add_argument('window-size=1920x1080')\n",
    "    driver = webdriver.Firefox(options=firefox_options)\n",
    "    \n",
    "    try:\n",
    "        driver.get(my_url)\n",
    "        time.sleep(random.randrange(5, 10))\n",
    "        \n",
    "        # Attempt to close the pop-up window\n",
    "        try:\n",
    "            close_popup = driver.find_element(By.CSS_SELECTOR, 'button.u-right-2x.u-top-2x.u-absolute.u-z-2.hover\\\\:u-opacity-50')\n",
    "            close_popup.click()\n",
    "            print(\"Pop-up closed\")\n",
    "        except Exception:\n",
    "            print(\"No pop-up found or error closing pop-up:\")\n",
    "        \n",
    "        # Allow additional time for the page to stabilize after closing the pop-up\n",
    "        time.sleep(2)\n",
    "        \n",
    "        page_contents = driver.page_source\n",
    "    except NoSuchWindowException:\n",
    "        print(\"Browser window was lost. Unable to scrape:\", my_url)\n",
    "        page_contents = \"\"\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    if page_contents:\n",
    "        return BeautifulSoup(page_contents, 'html.parser')\n",
    "    else:\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3feddf0",
   "metadata": {},
   "source": [
    "# GET LINKS TO INDIVIDUAL ITEM PAGES FROM MAIN PAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "pleased-hello",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KSN: Page 0...\n",
      "https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=0\n",
      "Scraping URL: https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=0\n",
      "No pop-up found or error closing pop-up:\n",
      "KSN: Page 1...\n",
      "https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=1\n",
      "Scraping URL: https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=1\n",
      "No pop-up found or error closing pop-up:\n",
      "KSN: Page 2...\n",
      "https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=2\n",
      "Scraping URL: https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=2\n",
      "Pop-up closed\n",
      "KSN: Page 3...\n",
      "https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=3\n",
      "Scraping URL: https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=3\n",
      "Pop-up closed\n",
      "KSN: Page 4...\n",
      "https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=4\n",
      "Scraping URL: https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=4\n",
      "Pop-up closed\n",
      "KSN: Page 5...\n",
      "https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=5\n",
      "Scraping URL: https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=5\n",
      "Pop-up closed\n",
      "KSN: Page 6...\n",
      "https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=6\n",
      "Scraping URL: https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=6\n",
      "Pop-up closed\n",
      "KSN: Page 7...\n",
      "https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=7\n",
      "Scraping URL: https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=7\n",
      "Pop-up closed\n",
      "KSN: Page 8...\n",
      "https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=8\n",
      "Scraping URL: https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=8\n",
      "Pop-up closed\n",
      "KSN: Page 9...\n",
      "https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=9\n",
      "Scraping URL: https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=9\n",
      "Pop-up closed\n",
      "KSN: Page 10...\n",
      "https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=10\n",
      "Scraping URL: https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=10\n",
      "Pop-up closed\n",
      "KSN: Page 11...\n",
      "https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=11\n",
      "Scraping URL: https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=11\n",
      "Pop-up closed\n",
      "KSN: Page 12...\n",
      "https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=12\n",
      "Scraping URL: https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=12\n",
      "Pop-up closed\n",
      "KSN: Page 13...\n",
      "https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=13\n",
      "Scraping URL: https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=13\n",
      "Pop-up closed\n",
      "KSN: Page 14...\n",
      "https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=14\n",
      "Scraping URL: https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=14\n",
      "Pop-up closed\n",
      "KSN: Page 15...\n",
      "https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=15\n",
      "Scraping URL: https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=15\n",
      "Pop-up closed\n",
      "KSN: Page 16...\n",
      "https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=16\n",
      "Scraping URL: https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=16\n",
      "Pop-up closed\n",
      "KSN: Page 17...\n",
      "https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=17\n",
      "Scraping URL: https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=17\n",
      "Pop-up closed\n",
      "KSN: Page 18...\n",
      "https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=18\n",
      "Scraping URL: https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=18\n",
      "Pop-up closed\n",
      "KSN: Page 19...\n",
      "https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=19\n",
      "Scraping URL: https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=19\n",
      "Pop-up closed\n",
      "KSN: Page 20...\n",
      "https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=20\n",
      "Scraping URL: https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=20\n",
      "Pop-up closed\n",
      "KSN: Page 21...\n",
      "https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=21\n",
      "Scraping URL: https://www.thredup.com/women/dresses?category_tags=dresses&department_tags=women&sort=newest_first&page=21\n",
      "Pop-up closed\n",
      "Product links found\n"
     ]
    }
   ],
   "source": [
    "def get_item_links(front_page_url, max_pages = 1):\n",
    "    product_links = []\n",
    "    # Everytime range increases, items increase by 50.\n",
    "    for page_number in range(max_pages + 1):\n",
    "        print(f\"KSN: Page {page_number}...\")\n",
    "        #See if page exists\n",
    "        try:\n",
    "            url_page = front_page_url[:-1] + str(page_number)\n",
    "            print(url_page)\n",
    "            main_page_items = scrape_page(url_page)\n",
    "        except:\n",
    "            print('exception')\n",
    "            break #exit for loop if a page doesn't exist (presumably means past last page for this item) \n",
    "        \n",
    "        if main_page_items:\n",
    "            #Pull all href links\n",
    "            url_front = \"https://www.thredup.com\"\n",
    "            all_products = main_page_items.find_all(attrs={\"data-inp-label\": \"link-item-card\"})\n",
    "\n",
    "\n",
    "            for product in all_products: #get all product links\n",
    "                product_link = url_front + product[\"href\"]\n",
    "                product_links.append(product_link)\n",
    "                \n",
    "    return product_links\n",
    "\n",
    "            \n",
    "product_links = get_item_links(front_page_url, max_pages)\n",
    "\n",
    "\n",
    "print(f\"Product links found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1c5c7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2618"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(product_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40c0f9f",
   "metadata": {},
   "source": [
    "# FUNCTIONS TO SCRAPE THE DETAILS of ITEMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a002f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Product Id from product page (prepare function)\n",
    "\n",
    "def get_product_id(product_link):\n",
    "    product_id = product_link.split(\"?query\")[0]   \n",
    "    product_id = product_id.split(\"/\")[-1]\n",
    "    \n",
    "    return product_id\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0cc52bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get product image from product page (prepare functions)\n",
    "\n",
    "def get_image_link(scraped_page):\n",
    "    try:\n",
    "        images = scraped_page.findAll(\n",
    "            \"img\", {\"class\": lambda L: L and L.startswith(\"u-rounded-4 u-cursor-pointer\")} \n",
    "        )\n",
    "        \n",
    "        image_link = images[0].get(\"src\")\n",
    "        print(image_link)\n",
    "    except Exception:\n",
    "        print(\"No image link found\")\n",
    "        image_link = \"\"\n",
    "        \n",
    "    return image_link\n",
    "\n",
    "def save_image(image_link, product_id, save_folder):\n",
    "    filename = save_folder + \"item\" + product_id +\".jpg\"\n",
    "\n",
    "    # Open the url image, set stream to True, this will return the stream content.\n",
    "    r = requests.get(image_link, stream = True)\n",
    "\n",
    "     # Check if the image was retrieved successfully\n",
    "    if r.status_code == 200:\n",
    "        # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "        r.raw.decode_content = True\n",
    "        # Open a local file with wb ( write binary ) permission.\n",
    "        with open(filename,'wb') as f:\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "        print(\"image saved\")\n",
    "    else:\n",
    "        print('Image Couldn\\'t be retrieved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93635c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size_and_brand(scraped_page):\n",
    "    size = scraped_page.find_all(\"div\", attrs={\"class\": \"u-text-16\"}) #structure of find_all input is (element, attrs={})\n",
    "    brand = size[0].parent.find_previous_sibling().find(\"a\")[\"title\"]\n",
    "    size = size[0].text\n",
    "    # print(size)\n",
    "    # print(brand)\n",
    "    return size, brand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "675e31a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category(scraped_page):\n",
    "    category = scraped_page.find_all(\"span\", attrs={\"class\": \"u-text-16\"}) #structure of find_all input is (element, attrs={})\n",
    "    category = category[0].text\n",
    "    # print(category)\n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "070eadd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price(scraped_page):\n",
    "    price = scraped_page.find_all(\"span\", attrs={\"class\": \"price\"}) \n",
    "    price = price[0].text\n",
    "    # print(price)\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc72a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_condition(scraped_page):\n",
    "    condition_header = scraped_page.find(\"h2\", string=\"Condition\")\n",
    "    condition = condition_header.find_next_sibling()\n",
    "    condition = condition.text\n",
    "    # print(condition)\n",
    "    return condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "893af627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_material_and_features(scraped_page):\n",
    "    details_header = scraped_page.find(\"h2\", string=\"Item details\")\n",
    "    details = details_header.parent.find_next_sibling().find_all('li')\n",
    "    material = details[0].text\n",
    "    features = details[1].text\n",
    "    \n",
    "    # print(material)\n",
    "    # print(features)\n",
    "\n",
    "    return material, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4750cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_measurements_and_fit(scraped_page):\n",
    "    try:\n",
    "        measurements_and_fit_header = scraped_page.find(\"h2\", string=\"Size & fit\")\n",
    "        measurements_and_fit = measurements_and_fit_header.find_next_sibling().find_all('li')\n",
    "        measurements = measurements_and_fit[0].text\n",
    "        measurements = measurements.replace(\"How we measure\",\"\")\n",
    "        fit = measurements_and_fit[1].text\n",
    "    except:\n",
    "        measurements = None\n",
    "        fit = None\n",
    "    \n",
    "    # print(measurements)\n",
    "    # print(fit)\n",
    "\n",
    "    return measurements, fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a772d5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json_files(product, product_id, save_folder):\n",
    "    filename = save_folder + \"json_files/item\" + product_id +\".json\"\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(product, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7097f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cfdd8d1",
   "metadata": {},
   "source": [
    "# Get all product info from product page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f175d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_info(product_link, save_folder):\n",
    "    \"\"\"Extract and save product information from a given URL.\n",
    "    \n",
    "    Args:\n",
    "        product_link (str): URL of the product page to scrape.\n",
    "        save_folder (str): Directory path to save the product's image and JSON data.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing product details or None if an error occurs or data cannot be scraped.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        scraped_page = scrape_page(product_link)\n",
    "        if not scraped_page:\n",
    "            return None\n",
    "        \n",
    "        product_id = get_product_id(product_link)\n",
    "        image_link = get_image_link(scraped_page)\n",
    "        if not image_link:\n",
    "            return None\n",
    "\n",
    "        save_image(image_link, product_id, save_folder)\n",
    "        size, brand = get_size_and_brand(scraped_page)\n",
    "        category = get_category(scraped_page)\n",
    "        condition = get_condition(scraped_page)\n",
    "        material, features = get_material_and_features(scraped_page)\n",
    "        measurements, fit = get_measurements_and_fit(scraped_page)\n",
    "\n",
    "        product_dict = {\n",
    "            \"product_link\": product_link,\n",
    "            \"product_id\": product_id,\n",
    "            \"size\": size,\n",
    "            \"brand\": brand,\n",
    "            \"category\": category,\n",
    "            \"condition\": condition,\n",
    "            \"material\": material,\n",
    "            \"features\": features,\n",
    "            \"measurements\": measurements,\n",
    "            \"fit\": fit\n",
    "        }\n",
    "\n",
    "        save_json_files(product_dict, product_id, save_folder)\n",
    "        return product_dict\n",
    "    except Exception as e:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8e29b6",
   "metadata": {},
   "source": [
    "# PRODUCT LOOP TO GET ALL OF EACH PRODUCT'S INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66c3234",
   "metadata": {},
   "outputs": [],
   "source": [
    "if max_products == \"all\":\n",
    "    max_products = len(product_links)\n",
    "\n",
    "products = []\n",
    "for product_link in tqdm(product_links[:max_products]):\n",
    "    print(f\"--------------\\ngetting info for product: {product_link}\")\n",
    "    product = get_product_info(product_link, save_folder)\n",
    "    if product:\n",
    "        products.append(product)\n",
    "    #Pause for random duration to not trigger bot blocker\n",
    "    time.sleep(random.randrange(5, 10))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4bcbdf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save json\n",
    "\n",
    "# Specify the filename\n",
    "filename = 'scraped_product_details.json'\n",
    "\n",
    "with open(save_folder + filename, 'w') as f:\n",
    "    json.dump(products, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
